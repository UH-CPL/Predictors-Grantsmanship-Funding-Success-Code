---
title: "Ordinal Logistic Regression or Proportional Odds Logistic Regression"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(GGally)
library(reshape2)
library(plyr)
library(nnet)
library(MASS)
library(caret)
library(mlbench)
library(rms)
# install.packages("rms")
# knitr::opts_chunk$set(echo = TRUE)

#######----Source----------#######

####https://www.youtube.com/watch?v=Z5WKQr4H4Xk
####https://www.youtube.com/watch?v=qkivJzjyHoA
####https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/
####https://www.youtube.com/watch?v=zE7pVAalmfk&feature=youtu.be&t=424
####http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/
# DWH = Workload a week before proposal deadline

# Deviance is a measure of goodness of fit of a model. Higher numbers always indicates bad fit.
# Akaike information criterion (AIC) (Akaike, 1974) is a fined technique based on in-sample fit to estimate the likelihood of a model to predict/estimate the future values. A good model is the one that has minimum AIC among all the other models. ... A lower AIC or BIC value indicates a better fit.


```



```{r, echo=FALSE}

root_dir <- getwd()
project_directory <- dirname(root_dir)
data_dir <- file.path(project_directory, 'raw-data')
curated_data_dir <- file.path(project_directory, 'curated-data')
plot_dir <- file.path(project_directory, 'Plots')
data_file_name <- 'CorrelationData.csv'
data_file_name_std <- 'StandardPart.csv'


Data <-
  read.csv(file.path(curated_data_dir, data_file_name),
           stringsAsFactors = FALSE)
Data <- Data[complete.cases(Data), ]
```

```{r,echo=FALSE, warning=FALSE}
success_rate <- ggplot(Data, aes(x = SR)) +
  geom_histogram(binwidth = .5, stat = "count") + theme_minimal() +
  ylim(0, 100) +
  ylab("Count") + xlab("Success rate") + scale_x_discrete(
    limits = c("1", "2", "3", "4", "5", "6", "7"),
    labels = c("< 10%",
               "10-20%",
               "20-30%",
               "30-50%",
               "50-75%",
               "75-90%",
               "> 90%")
  ) + ggtitle("Proposals success rate") + theme(plot.title = element_text(hjust = 0.5))
print(success_rate)


# success_rate <- ggplot(Data, aes(x = SR)) +
#   geom_histogram(binwidth = .5, stat = "count") + theme_minimal() +
#   ylim(0, 180) +
#   ylab("Count") + xlab("Success rate") + scale_x_discrete(
#     limits = c("1", "2", "3"),
#     labels = c("10-30%",
#                "30-50%",
#                "50-100%")
#   ) + ggtitle("Proposals success rate") + theme(plot.title = element_text(hjust = 0.5))
# print(success_rate)
```




```{r, echo=FALSE}
Data$SR <- factor(Data$SR)
hindex = Data$H

lm_DF <- as.data.frame((Data[, 2:24]))


lm_DF$Rank <- as.factor(Data$Rank)
lm_DF$RS <- factor(lm_DF$RS)
lm_DF$WH <- factor(lm_DF$WH)
lm_DF$NP <- factor(lm_DF$NP)
lm_DF$H <- lm_DF$H + 0.001
lm_DF$H <- log(lm_DF$H)

lm_DF$DWH <- factor(lm_DF$DWH)
lm_DF$T <- factor(lm_DF$T)
lm_DF$DS <- factor(lm_DF$DS)



# ind<-sample(2,nrow(lm_DF), replace = TRUE, prob = c(0.8,0.2))
# train_data<-lm_DF[ind==1,]
# test_data<-lm_DF[ind==2,]
```
\newpage
## Model with NP1 as reference
```{r, echo=FALSE, warning=FALSE}
# lm_DF$SR <- relevel(lm_DF$SR, ref = 7)
base_model = polr(SR ~ H + NP + AGR + TA + RS + DWH + DS, data = lm_DF, Hess = TRUE)
# base_model=lrm(SR ~ H + NP + AGR, data = lm_DF)
summary(base_model)

ctable <- coef(summary(base_model))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = round(p, 4))
ctable
```

```{r, echo=FALSE, warning=FALSE}
# \newpage
## Model with NP1 as reference and kfold cross validation

# set.seed(123)
# fitcontrol<- trainControl(method = "CV", number = 10)
# 
# # mod_fitcv<-train(SR ~ H + AGR + NP + DWH  + T + DS, data = lm_DF, na.action = na.omit , method="polr", trControl = fitcontrol)
# mod_fitcv<-train(SR ~ H + AGR + NP, data = lm_DF, na.action = na.omit , method="polr", trControl = fitcontrol)
# # print(mod_fitcv)
# # mod_fitcv<-clm(SR ~ H + AGR + NP + DWH  + T + DS, data = lm_DF, Hess = TRUE)
# summary(mod_fitcv)
# 
# ctable <- coef(summary(mod_fitcv))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# ctable <- cbind(ctable, "p value" = round(p,4))
# ctable

```


```{r, echo=FALSE}
# pred <- predict(mod_fitcv, lm_DF)
# # pred
# tab<-table(pred, lm_DF$SR)
# tab
# result<- 1- sum(diag(tab))/sum(tab)
# result
```



\newpage
## Model with NP3 as reference 
```{r, echo=FALSE, warning=FALSE}
lm_DF$NP <- relevel(lm_DF$NP, ref = "3")
base_model = polr(SR ~ H + NP + AGR + TA + RS + DWH + DS, data = lm_DF, Hess = TRUE)
summary(base_model)
ctable <- coef(summary(base_model))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = round(p, 4))
ctable
```


```{r, echo=FALSE, warning=FALSE}
#\newpage
## Model with NP4 as reference and kfold cross validation

# set.seed(123)
# fitcontrol<- trainControl(method = "CV", number = 10, savePredictions = T)
# 
# # mod_fitcv<-train(SR ~ H + AGR + NP, data = lm_DF , method="polr", trControl = fitcontrol)
# mod_fitcv<-train(SR ~ H + AGR + NP, data = lm_DF , method="polr", trControl = fitcontrol)
# summary(mod_fitcv)
# 
# ctable <- coef(summary(mod_fitcv))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# ctable <- cbind(ctable, "p value" = round(p,4))
# ctable

```


\newpage
## Model with all variables NP1, DWH5 and DS1 as reference
```{r, echo=FALSE, warning=FALSE}

lm_DF$NP <- relevel(lm_DF$NP, ref = "1")
lm_DF$DWH <- relevel(lm_DF$DWH, ref = "5")

mod_fitcv <-
  polr(SR ~ H + NP + AGR + TA + RS + DWH + DS, data = lm_DF, Hess = TRUE)
# mod_fitcv<-polr(SR ~ DS, data = lm_DF , Hess = TRUE)
summary(mod_fitcv)

ctable <- coef(summary(mod_fitcv))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = round(p, 4))
ctable
```


\newpage
## Model with all variables, NP3, DWH1 and DS1 as reference
```{r, echo=FALSE, warning=FALSE}

# lm_DF$DS<-relevel(lm_DF$DS, ref = "1")
lm_DF$NP <- relevel(lm_DF$NP, ref = "3")
lm_DF$DWH <- relevel(lm_DF$DWH, ref = "1")

mod_fitcv <-
  polr(SR ~ H + NP + AGR + TA + RS + DWH + DS, data = lm_DF, Hess = TRUE)
# mod_fitcv<-polr(SR ~ DS, data = lm_DF , Hess = TRUE)
summary(mod_fitcv)

ctable <- coef(summary(mod_fitcv))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = round(p, 4))
ctable
```

```{r, echo=FALSE, warning=FALSE}
# \newpage
# ## Model with all variables and Kfold cross validation
# 
# set.seed(123)
# fitcontrol<- trainControl(method = "CV", number = 5, savePredictions = T)
# 
# mod_fitcv2<-train(SR ~ H + AGR + NP + DWH  + DS, data = lm_DF , method="polr", trControl = fitcontrol)
# summary(mod_fitcv2)
# 
# ctable <- coef(summary(mod_fitcv2))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# ctable <- cbind(ctable, "p value" = round(p,4))
# ctable

```

```{r, echo=FALSE, warning=FALSE}
# model<-polr(SR ~ H + AGR + NP + DWH  + T + DS, data = lm_DF, Hess = TRUE)
# summary(model)
# ctable <- coef(summary(model))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# ctable <- cbind(ctable, "p value" = round(p,4))
# ctable
```


```{r, echo=FALSE}

# \newpage
# ## Cooccurrence matrix and misclassification rate
# pred <- predict(mod_fitcv, lm_DF)
# # pred
# tab<-table(pred, lm_DF$SR)
# tab
# result<- 1- sum(diag(tab))/sum(tab)
# result
```

```{r, echo=FALSE}

## Prediction on testing data
### Cooccurrence matrix and misclassification rate on training data
# pred1 <- predict(mod_fitcv, test_data)
# # pred
# tab1<-table(pred1, test_data$SR)
# tab1
# result<- 1- sum(diag(tab1))/sum(tab1)
# result
```


```{r, echo=FALSE, warning=FALSE}

# lm_DF$NP<-relevel(lm_DF$NP, ref = "4")
# # lm_DF$DS<-relevel(lm_DF$DS, ref = "5")
# # lm_DF$T<-relevel(lm_DF$T, ref = "5")
# 
# set.seed(123)
# fitcontrol<- trainControl(method = "CV", number = 10, savePredictions = T)
# 
# mod_fitcv<-train(SR ~ H + AGR + NP + DWH  + T + DS, data = lm_DF , method="polr", trControl = fitcontrol)
# # summary(mod_fitcv)
# 
# ctable <- coef(summary(mod_fitcv))
# p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
# ctable <- cbind(ctable, "p value" = round(p,4))
# ctable

```


```{r, echo=FALSE}
# install.packages("randomForest")
# library(randomForest)
# 
# rand_model<- randomForest(SR ~ H + AGR + NP + DWH  + T + DS, data = lm_DF)
# rand_model
# 
# summary(rand_model)
```

